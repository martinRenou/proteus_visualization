{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `hdf5` file directly using memory mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import mmap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def metadata_to_array(metadata, mapping):\n",
    "    \"\"\"Turn array metadata into a NumPy array.\"\"\"\n",
    "    shape = metadata['shape']\n",
    "    dtype = metadata['dtype']\n",
    "    offset = metadata['offset']\n",
    "    length = np.prod(shape)\n",
    "\n",
    "    return np.frombuffer(mapping, dtype=dtype, count=length, offset=offset).reshape(shape)\n",
    "\n",
    "def extract_arrays_metadata(hdf5_path, print_metadata=False):\n",
    "    \"\"\"Extract arrays metadata from an HDF5 file.\"\"\"\n",
    "    arrays_metadata = {}\n",
    "\n",
    "    with h5py.File(hdf5_path, 'r') as fobj:\n",
    "        def dump(name, item):\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                if print_metadata:\n",
    "                    print(name, item.shape, item.dtype)\n",
    "\n",
    "                arrays_metadata[name] = dict(\n",
    "                    offset=item.id.get_offset(), \n",
    "                    shape=item.shape, \n",
    "                    dtype=item.dtype,\n",
    "                    filename=hdf5_path\n",
    "                )\n",
    "\n",
    "        fobj.visititems(dump)\n",
    "\n",
    "    return arrays_metadata\n",
    "\n",
    "def extract_array(arrays_metadata, array_name):\n",
    "    \"\"\"Extract NumPy array from an HDF5 file, given the arrays metadata and the array name you want to extract.\"\"\"\n",
    "    metadata = arrays_metadata[array_name]\n",
    "    \n",
    "    with open(metadata['filename'], 'rb') as fobj:\n",
    "        mapping = mmap.mmap(fobj.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "\n",
    "        return metadata_to_array(metadata, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_metadata = extract_arrays_metadata('reef/solitary_reef.h5')\n",
    "\n",
    "mem_vertices = extract_array(arrays_metadata, 'nodesSpatial_Domain0')\n",
    "vertices = np.array(mem_vertices[:, 0:2])\n",
    "\n",
    "indices = extract_array(arrays_metadata, 'elementsSpatial_Domain0')\n",
    "\n",
    "h = extract_array(arrays_metadata, 'h_t0')\n",
    "bathymetry = extract_array(arrays_metadata, 'bathymetry0_t0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays_metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- compute underwater component DONE\n",
    "- manual threshold effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipygany import Scene, Data, Component, PolyMesh, Water, UnderWater, Data, Component, Threshold, Warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_value = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = extract_array(arrays_metadata, 'h_t0')\n",
    "\n",
    "h_component = Component(name='h', array=h)\n",
    "\n",
    "z_water = h + bathymetry\n",
    "\n",
    "water_mesh = PolyMesh(\n",
    "    vertices=np.append(vertices, z_water.reshape((z_water.shape[0], 1)) * warp_value, axis=1),\n",
    "    triangle_indices=indices,\n",
    "    data=[Data(name='h', components=[h_component])]\n",
    ")\n",
    "\n",
    "actual_water = Threshold(water_mesh, input='h', min=0.0000000000001, max=1000)\n",
    "\n",
    "floor = PolyMesh(\n",
    "    vertices=np.append(vertices, bathymetry.reshape((bathymetry.shape[0], 1)) * warp_value, axis=1),\n",
    "    triangle_indices=indices,\n",
    "    data=[Data(name='underwater', components=[h_component])]\n",
    ")\n",
    "\n",
    "water = Water(\n",
    "    actual_water, \n",
    "    under_water_blocks=(UnderWater(floor), ),\n",
    "    caustics_enabled=True,\n",
    ")\n",
    "\n",
    "scene = Scene((water, ))\n",
    "\n",
    "scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water.caustics_factor = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in range(249):\n",
    "    h = extract_array(arrays_metadata, 'h_t{}'.format(i))\n",
    "\n",
    "    h_component.array = h\n",
    "\n",
    "    z_water = h + bathymetry\n",
    "\n",
    "    water_mesh.vertices = np.append(vertices, z_water.reshape((z_water.shape[0], 1)) * warp_value, axis=1)\n",
    "\n",
    "    sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
